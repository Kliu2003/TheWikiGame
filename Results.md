## General Setup

Before explaining how each of our algorithms were implemented, we will first describe how our class is setup. Our game class consists of three maps that allow us to create and optimize our algorithms. The first two maps are used for optimization purposes, allowing us to convert a Wikipedia link (string) to an article ID (integer) that we assign to each of the Wikipedia articles within our dataset. The two maps create a bijection between the links and the IDs, and because comparing integers is more efficient than comparing strings, we will convert our links to IDs before making comparisons within our algorithms. Finally, our third map is an adjacency list for each of our article IDs, with the key being the article ID and the value being a vector of article IDs. This vector will hold all the articles that you can get to through the page associated with the article ID. The graph generated will be a directed graph, as just because there is a link on page A that leads to page B, it does not necessarily mean there is a link on page B that leads back to page A.

## Breadth First Search

Our first/traversal algorithm uses BFS to find the shortest path between two articles. In this case, we define the shortest path as the least amount of clicks/page traversals required to get from the starting page to the ending page. We first check to make sure both our starting link and ending link are valid Wikipedia links within our dataset. We then need to keep track of pages that we've already visited so we don't get stuck in an endless loop and we need a boolean to determine whether we have found a path or not. Finally, to get the series of pages of our shortest path, we use a similar idea to union find where instead of keeping track of the parent node, we keep track of the previous page ID of our traversal so we can backtrack later. We then perform a typical BFS algorithm using a queue and the adjacency list map, and when the adjacent ID is found to be the same as our end ID, then we know we've found a valid path, and we update the found path boolean value to true to signify we have found a path. If the queue empties before we find a path, then we know that the path does not exist, so we keep our found boolean as false. If we could not find a path, then we let the user know that it is impossible to travel between the two pages. If there is a path, we use our union find array to trace the path from the end to the start and add each article to the final vector we return.

## Dijkstra's Algorithm

Our second algorithm uses Dijkstra's to find the smallest path (by weight) between two specified starting and ending locations. For this algorithm, we define the smallest path as that which follows the pages with most outbound links. Instead of printing the path with the least number of clicks required to reach the end (as we have already done in BFS), we print out the paths a player new to the wiki game is likely to choose (the most common pages, aka the ones with the most links). This also provides more options, and generally does a decent job of finding destination links as fast as possible. To do so, we keep track of the best distances by weight it takes to reach each link (also referred to as a node in the following), and explore the nodes connected to it in the adjacency list. The weights are calculated as the negative of the number of nodes each node directs edges to (because we are minimizing weight; could also maximize). We store the weights and nodes in a pair in a set so there are no duplicates and the STL set organizes itself by weight, which is the first element of the std::pair. We also keep track of a visited array, so if nodes have been visited, they are not traversed again. The way to print out a path for Djikstra's is to keep a "previous" array, which can be used at the end to backtrack the path by which we reached the final destination with the smallest path. We then simply store the results in a vector and return it (and print it out to the terminal). If we are unable to find a path connecting the two pages, we return a vector with "no path found." as the only element inside it.

## Page Rank

Our final algorithm is a PageRank algorithm that was first conceptualized for webpage browsing, but also works for Wikipedia articles, as it is inherent that some articles are more popular than others. The PageRank algorithm first assumes that all Wikipedia pages have an equal "page score" and the probability of moving any other accessible page is equal. So, we distribute the score of the current page equally amongst all the other pages on each "cycle" based on the current score. After many iterations of this concept, we will eventually arrive with a new set of scores that represent are weighed on how "popular" the page is. The more iterations we run, the more accurate our PageRank will be. Eventually, pages that are unaccessible will have a score of 0, while pages such as "Abraham Lincoln" will have a high score because it is a common page. The algorithm implemented also applies a damping factor, which accounts for the idea that a user will not browse the Wikipedia network forever. The damping factor is the probability that on a given page, the user will not click on another link. The output of this algorithm will be a vector of pairs sorted by PageRank scores, with each pair having the page ID and associated PageRank score.
